# =============================================================================
#
#         è„šæœ¬ 1: BERTopic æ¨¡å‹è®­ç»ƒä¸ä¿å­˜ (å·²ä¿®æ­£)
#
#   åŠŸèƒ½:
#   1. åŠ è½½æ•°æ®å¹¶ä¸º Bilibili å’Œ YouTube å¹³å°åˆ†åˆ«è®­ç»ƒ BERTopic æ¨¡å‹ã€‚
#   2. è®­ç»ƒæˆåŠŸåï¼Œä¿å­˜ä¸¤ä»½äº§å‡ºï¼š
#      - ä¸»é¢˜ä¿¡æ¯çš„ .xlsx æ–‡ä»¶ã€‚
#      - å®Œæ•´çš„ BERTopic æ¨¡å‹å¯¹è±¡æ–‡ä»¶å¤¹ (ç”¨äºåç»­å¯è§†åŒ–)ã€‚
#
# =============================================================================

import pandas as pd
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
import os
from sentence_transformers import SentenceTransformer
from bertopic.vectorizers import ClassTfidfTransformer

# --- ğŸš€ã€è¯·åœ¨è¿™é‡Œé…ç½®æ‚¨çš„è·¯å¾„ã€‘ğŸš€ ---
INPUT_FILE_PATH = 'data/comments_for_bertopic.xlsx'
EMBEDDING_MODEL_NAME = 'shibing624/text2vec-base-chinese'
PLATFORM_COLUMN = 'platform'
EMBEDDING_TEXT_COLUMN = 'cleaned_text_for_sentiment'
TOPIC_WORDS_COLUMN = 'text_for_bertopic'
OUTPUT_DIR_RESULTS = 'results/topic_modeling/'
# ----------------------------------------------------

# ã€æ–°å¢ã€‘=======================================================================
# å®šä¹‰ä¸€ä¸ªé¡¶å±‚å‡½æ•°æ¥æ›¿ä»£ lambdaï¼Œä»¥è§£å†³ PicklingErrorã€‚
# Pickle æ¨¡å—å¯ä»¥æ­£ç¡®åœ°åºåˆ—åŒ–å’Œååºåˆ—åŒ–è¿™ä¸ªæœ‰æ˜ç¡®è·¯å¾„çš„å‡½æ•°ã€‚
def split_tokenizer(text):
    """ä¸€ä¸ªç®€å•çš„åˆ†è¯å™¨ï¼Œé€šè¿‡ç©ºæ ¼åˆ†å‰²æ–‡æœ¬ã€‚"""
    return text.split()
# ===============================================================================

def main():
    print("\n--- å¯åŠ¨ã€è„šæœ¬ 1: æ¨¡å‹è®­ç»ƒä¸ä¿å­˜ã€‘---")
    os.makedirs(OUTPUT_DIR_RESULTS, exist_ok=True)

    print(f"\n> æ­£åœ¨ä» '{INPUT_FILE_PATH}' åŠ è½½æ•°æ®...")
    try:
        df = pd.read_excel(INPUT_FILE_PATH)
    except FileNotFoundError:
        print(f"  > [ä¸¥é‡é”™è¯¯] æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: '{INPUT_FILE_PATH}'ã€‚ç¨‹åºç»ˆæ­¢ã€‚")
        return
        
    df.dropna(subset=[EMBEDDING_TEXT_COLUMN, TOPIC_WORDS_COLUMN], inplace=True)
    print(f"  > æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(df)} æ¡æœ‰æ•ˆè¯„è®ºã€‚")

    df_bili = df[df[PLATFORM_COLUMN].str.contains('bilibili', case=False, na=False)]
    df_yt = df[df[PLATFORM_COLUMN].str.contains('youtube', case=False, na=False)]
    print(f"  > æ•°æ®æ‹†åˆ†å: Bilibili {len(df_bili)} æ¡, YouTube {len(df_yt)} æ¡ã€‚")

    print(f"\n> æ­£åœ¨é¢„åŠ è½½å¥å­æ¨¡å‹: '{EMBEDDING_MODEL_NAME}' (å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)

    # --- Bilibili å¹³å°å¤„ç† ---
    if len(df_bili) > 50:
        print("\n--- [1/2] æ­£åœ¨ä¸º Bilibili å¹³å°å»ºæ¨¡... ---")
        topic_model_bili = run_bertopic_for_platform(df_bili, embedding_model, platform="bilibili")
        if topic_model_bili:
            save_all_results(topic_model_bili, 'bilibili')
    else:
        print("\n--- [1/2] Bilibili æ•°æ®é‡ä¸è¶³ (<50æ¡)ï¼Œè·³è¿‡å»ºæ¨¡ã€‚")

    # --- YouTube å¹³å°å¤„ç† ---
    if len(df_yt) > 50:
        print("\n--- [2/2] æ­£åœ¨ä¸º YouTube å¹³å°å»ºæ¨¡... ---")
        topic_model_yt = run_bertopic_for_platform(df_yt, embedding_model, platform="youtube")
        if topic_model_yt:
            save_all_results(topic_model_yt, 'youtube')
    else:
        print("\n--- [2/2] YouTube æ•°æ®é‡ä¸è¶³ (<50æ¡)ï¼Œè·³è¿‡å»ºæ¨¡ã€‚")

    print("\n--- è„šæœ¬ 1 æ‰§è¡Œå®Œæ¯•ï¼æ‰€æœ‰æ¨¡å‹å’Œç»“æœå·²ä¿å­˜ã€‚---")

def run_bertopic_for_platform(df_platform, embedding_model_object, platform):
    docs_for_embedding = df_platform[EMBEDDING_TEXT_COLUMN].tolist()
    docs_for_topic_words = df_platform[TOPIC_WORDS_COLUMN].tolist()
    
    # ã€ä¿®æ”¹å¤„ã€‘===================================================================
    # ä½¿ç”¨æˆ‘ä»¬å®šä¹‰çš„é¡¶å±‚å‡½æ•° split_tokenizer æ›¿ä»£åŸæ¥çš„ lambda å‡½æ•°ã€‚
    vectorizer_model = CountVectorizer(tokenizer=split_tokenizer, min_df=2)
    # ===============================================================================
    
    min_topic_size = 20
    ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)

    topic_model = BERTopic(
        embedding_model=embedding_model_object, vectorizer_model=vectorizer_model,
        ctfidf_model=ctfidf_model, language="multilingual", nr_topics="auto",
        min_topic_size=min_topic_size, calculate_probabilities=True, verbose=True
    )
    
    try:
        print(f"  > å¹³å°: {platform}, å‚æ•°: min_topic_size={min_topic_size}, BM25å¼€å¯")
        embeddings = embedding_model_object.encode(docs_for_embedding, show_progress_bar=True)
        topics, probs = topic_model.fit_transform(docs_for_topic_words, embeddings)
        print(f"  > {platform} å¹³å°å»ºæ¨¡å®Œæˆï¼Œå‘ç° {len(topic_model.get_topic_info())-1} ä¸ªä¸»é¢˜ã€‚")
        return topic_model
    except Exception as e:
        print(f"  > [é”™è¯¯] åœ¨ä¸º {platform} å¹³å°å»ºæ¨¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def save_all_results(topic_model, platform_name):
    """ç»Ÿä¸€ä¿å­˜Excelæ‘˜è¦å’Œå®Œæ•´çš„æ¨¡å‹å¯¹è±¡"""
    # 1. ä¿å­˜ä¸»é¢˜ä¿¡æ¯ Excel
    topic_info_df = topic_model.get_topic_info()
    excel_path = os.path.join(OUTPUT_DIR_RESULTS, f'{platform_name}_topics_info.xlsx')
    topic_info_df.to_excel(excel_path, index=False)
    print(f"  > ä¸»é¢˜ä¿¡æ¯è¡¨å·²ä¿å­˜è‡³: '{os.path.basename(excel_path)}'")
    
    # 2. ä¿å­˜å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶å¤¹
    model_path = os.path.join(OUTPUT_DIR_RESULTS, f'{platform_name}_bertopic_model')
    
    # ã€ä¼˜åŒ–å¤„ã€‘ æ¨èä½¿ç”¨ safetensors æ ¼å¼ä¿å­˜ï¼Œå› ä¸ºå®ƒæ›´å®‰å…¨ã€æ›´ç°ä»£ã€‚
    # å¦‚æœå¤±è´¥ï¼Œä¼šè‡ªåŠ¨å›é€€åˆ° pickle æ–¹æ³•ã€‚
    try:
        topic_model.save(model_path, serialization="safetensors")
        print(f"  > å®Œæ•´BERTopicæ¨¡å‹å·²ä¿å­˜è‡³: '{os.path.basename(model_path)}' (ä½¿ç”¨ safetensors)")
    except Exception as e:
        print(f"  > [æç¤º] ä½¿ç”¨ safetensors ä¿å­˜å¤±è´¥ ({e})ï¼Œå°è¯•ä½¿ç”¨ pickle æ–¹æ³•...")
        topic_model.save(model_path, serialization="pickle")
        print(f"  > å®Œæ•´BERTopicæ¨¡å‹å·²ä¿å­˜è‡³: '{os.path.basename(model_path)}' (ä½¿ç”¨ pickle)")


if __name__ == "__main__":
    main()